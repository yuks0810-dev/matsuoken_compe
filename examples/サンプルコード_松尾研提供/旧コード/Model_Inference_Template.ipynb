{"cells":[{"cell_type":"markdown","metadata":{"id":"srVbrpLVr42E"},"source":["# 推論用コード\n","Hugging Faceにアップロードしたモデルを用いてELYZA-tasks-100-TVの出力を得るためのコードです。  \n","このコードで生成されたjsonlファイルは課題の成果として提出可能なフォーマットになっております。\n","unslothで学習したモデル"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bf7acpcojFCG"},"outputs":[],"source":["!pip install -U bitsandbytes\n","!pip install -U transformers\n","!pip install -U accelerate\n","!pip install -U datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbHRiK-Hr42I"},"outputs":[],"source":["# notebookでインタラクティブな表示を可能とする（ただし、うまく動かない場合あり）\n","!pip install ipywidgets --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ruyjtkR2Vha-"},"outputs":[],"source":["from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n",")\n","import torch\n","from tqdm import tqdm\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qVObtsDvYCeo"},"outputs":[],"source":["# Hugging Faceで取得したTokenをこちらに貼る。\n","HF_TOKEN = \"Hugging Face Token\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zAGpSu7JVjOU"},"outputs":[],"source":["# 自分の作成したモデルのIDをこちらに貼る。\n","model_name = \"YOUR FINETUNED MODEL\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZwE8NV9OVko7"},"outputs":[],"source":["# QLoRA config\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_use_double_quant=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8PxN4bPni715"},"outputs":[],"source":["# Load model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n","    token = HF_TOKEN\n",")\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token = HF_TOKEN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRc6CJI2i9J9"},"outputs":[],"source":["# データセットの読み込み。\n","# omnicampusの開発環境では、左にタスクのjsonlをドラッグアンドドロップしてから実行。\n","datasets = []\n","with open(\"./elyza-tasks-100-TV_0.jsonl\", \"r\") as f:\n","    item = \"\"\n","    for line in f:\n","      line = line.strip()\n","      item += line\n","      if item.endswith(\"}\"):\n","        datasets.append(json.loads(item))\n","        item = \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJLfoswbi_ON"},"outputs":[],"source":["# gemma\n","results = []\n","for data in tqdm(datasets):\n","\n","  input = data[\"input\"]\n","  prompt = f\"\"\"### 指示\n","  {input}\n","  ### 回答：\n","  \"\"\"\n","\n","  input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","  outputs = model.generate(**input_ids, max_new_tokens=512, do_sample=False, repetition_penalty=1.2,)\n","  output = tokenizer.decode(outputs[0][input_ids.input_ids.size(1):], skip_special_tokens=True)\n","\n","  results.append({\"task_id\": data[\"task_id\"], \"input\": input, \"output\": output})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JyrJvsFejB_U"},"outputs":[],"source":["# llmjp\n","results = []\n","for data in tqdm(datasets):\n","\n","  input = data[\"input\"]\n","\n","  prompt = f\"\"\"### 指示\n","  {input}\n","  ### 回答：\n","  \"\"\"\n","\n","  tokenized_input = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n","  with torch.no_grad():\n","      outputs = model.generate(\n","          tokenized_input,\n","          max_new_tokens=100,\n","          do_sample=False,\n","          repetition_penalty=1.2\n","      )[0]\n","  output = tokenizer.decode(outputs[tokenized_input.size(1):], skip_special_tokens=True)\n","\n","  results.append({\"task_id\": data[\"task_id\"], \"input\": input, \"output\": output})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCZ2dFkOeMyP"},"outputs":[],"source":["# こちらで生成されたjsolを提出してください。\n","# 本コードではinputとeval_aspectも含んでいますが、なくても問題ありません。\n","# 必須なのはtask_idとoutputとなります。\n","import re\n","model_name = re.sub(\".*/\", \"\", model_name)\n","with open(f\"./{model_name}-outputs.jsonl\", 'w', encoding='utf-8') as f:\n","    for result in results:\n","        json.dump(result, f, ensure_ascii=False)  # ensure_ascii=False for handling non-ASCII characters\n","        f.write('\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}