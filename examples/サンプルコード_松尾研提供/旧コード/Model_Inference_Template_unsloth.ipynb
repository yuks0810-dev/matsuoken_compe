{"cells":[{"cell_type":"markdown","metadata":{"id":"MljifiTVCT0_"},"source":["# 推論用コード\n","Hugging Faceにアップロードしたモデルを用いてELYZA-tasks-100-TVの出力を得るためのコードです。  \n","このコードはunslothライブラリを用いてモデルを読み込み、推論するためのコードとなります。\n","このコードで生成されたjsonlファイルは課題の成果として提出可能なフォーマットになっております。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5B5MOHuBy8b"},"outputs":[],"source":["%%capture\n","!pip install unsloth\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GM7SNRtACg9V"},"outputs":[],"source":["from unsloth import FastLanguageModel\n","import torch\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmdUATTVCtyr"},"outputs":[],"source":["model_name = \"あなたがFine-Tuningしたモデルの名前(パス)\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TB6Hzx-2B5g8"},"outputs":[],"source":["max_seq_length = 2048\n","dtype = None\n","load_in_4bit = True\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = model_name,\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    token = \"HF token\",\n",")\n","FastLanguageModel.for_inference(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fg_yURyiB8o6"},"outputs":[],"source":["# データセットの読み込み。\n","# omnicampusの開発環境では、左にタスクのjsonlをドラッグアンドドロップしてから実行。\n","datasets = []\n","with open(\"./elyza-tasks-100-TV_0.jsonl\", \"r\") as f:\n","    item = \"\"\n","    for line in f:\n","      line = line.strip()\n","      item += line\n","      if item.endswith(\"}\"):\n","        datasets.append(json.loads(item))\n","        item = \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwfZEra1CEJo"},"outputs":[],"source":["from tqdm import tqdm\n","\n","# 推論\n","results = []\n","for dt in tqdm(data):\n","  input = dt[\"input\"]\n","\n","  prompt = f\"\"\"### 指示\\n{input}\\n### 回答\\n\"\"\"\n","\n","  inputs = tokenizer([prompt], return_tensors = \"pt\").to(model.device)\n","\n","  outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True, do_sample=False, repetition_penalty=1.2)\n","  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).split('\\n### 回答')[-1]\n","\n","  results.append({\"task_id\": data[\"task_id\"], \"input\": input, \"output\": output})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"voAPnXp5CKRL"},"outputs":[],"source":["with open(f\"/content/{model_name}_output.jsonl\", 'w', encoding='utf-8') as f:\n","    for result in results:\n","        json.dump(result, f, ensure_ascii=False)\n","        f.write('\\n')"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}